<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
  <title>新世纪矩阵代数(大雾 | Shimushu</title>
  <link rel="icon" href="/img/avatar.png"/>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/mdui@1.0.1/dist/css/mdui.min.css"
    integrity="sha384-cLRrMq39HOZdvE0j6yBojO4+1PrHfB7a9l5qLcmRm/fiWXYY+CndJPmyu5FV/9Tw"
    crossorigin="anonymous"
  />
  <script
    src="https://cdn.jsdelivr.net/npm/mdui@1.0.1/dist/js/mdui.min.js"
    integrity="sha384-gCMZcshYKOGRX9r6wbDrvF+TcCCswSHFucUzUPwka+Gr+uHgjlYvkABr95TCOz3A"
    crossorigin="anonymous"
  ></script>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
    crossorigin="anonymous"
  />
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous">
  </script>
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
    crossorigin="anonymous"
    onload="renderMathInElement(document.body);">
  </script>
  <script defer src="/scripts/init.js" onload="readcookie();"></script>
</head>
<body class="mdui-appbar-with-toolbar mdui-drawer-body-left mdui-theme-layout-light mdui-theme-accent-deep-purple">
  <header class="mdui-appbar mdui-appbar-fixed mdui-appbar-scroll-hide">
    <div class="mdui-toolbar mdui-color-deep-purple">
      <a href="javascript:;" class="mdui-btn mdui-btn-icon" mdui-drawer="{target: '#main-drawer'}"><i class="mdui-icon material-icons">menu</i></a>
      <span class="mdui-typo-headline">nuntius ipsum</span>
    </div>
  </header>
  <div class="mdui-drawer" id="main-drawer" style="display: flex; flex-direction: column;">
    <ul class="mdui-list" style="flex: 1;">
      <li class="mdui-list-item mdui-ripple">
        <div class="mdui-list-item-avatar"><img src="/img/avatar.png"/></div>
        <div class="mdui-list-item-content">Shimushu</div>
      </li>
      <li class="mdui-divider"></li>
      <a class="mdui-list-item mdui-ripple" href="/" target="_self">
        <i class="mdui-list-item-icon mdui-icon material-icons">contact_mail</i>
        <div class="mdui-list-item-content">About me</div>
      </a>
      <a class="mdui-list-item mdui-ripple mdui-list-item-active" href="/blog.html" target="_self">
        <i class="mdui-list-item-icon mdui-icon material-icons">send</i>
        <div class="mdui-list-item-content">Blog</div>
      </a>
    </ul>
    <div class="mdui-typo" style="padding: 20px 16px;">
      © 2021 Shimushu<br/>
      Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0" target="_blank">CC BY-NC-SA 4.0</a><br/>
      Powered by <a href="https://mdui.org" target="_blank">MDUI</a>
    </div>
  </div>
  <div class="mdui-container mdui-typo">
    <h1>新世纪矩阵代数(大雾</h1>
    <p>
      在矩阵代数中, SVD(奇异值分解)可以说是一种非常强力的工具, 可以将任意一个复(实)矩阵 \(A^{m \times n}\) 分解成 \(U^{m \times m} \Sigma^{m \times n} (V^{*})^{n \times n}\) ,
      其中 \(U\) 和 \(V\) 都是酉(正交)矩阵, 而 \(\Sigma\) 仅在其主对角线上有非零元素, 这些对角线上的元素被称为 \(A\) 的奇异值. 并且, 更加神奇的是,
      这些奇异值就是 \(A^{*} A\) 的(当然也是 \(A A^{*}\) 的)特征值的平方根(取正或负的平方根是无所谓的).
    </p>
    <p>
      但是, 很可惜的是, 仿佛对于很多人而言, 对SVD的认知到这里就差不多结束了. 实际上, SVD还有很多特性, 以及相应的应用, 就比如有损压缩图片
      (虽然这绝不是最好的方法, 但是这是一种比较直观的对SVD的理解).
    </p>
    <h2>如何理解SVD?</h2>
    <p>
      那么我们就直入主题: SVD的三个矩阵是怎么还原出原来的矩阵的? 首先, 如果考虑将只在对角线上有非零元素的 \(\Sigma\) 乘在 \(U\) 的右边, 相当于对其做列变换,
      使得其每一列都乘以了对应列的奇异值, 多出来的列用 \(0\) 填充, 少掉的列直接砍掉. 或者, 将 \(\Sigma\) 乘在 \(V^{*}\) 的左边, 相当于对 \(V^{*}\) 作行变换,
      或者说, 是对原来的 \(V\) 作列变换, 然后再(共轭)转置回来. 所以我们就知道了 \(\Sigma\) 是用来放缩的.
      举个<a href="https://zh.wikipedia.org/zh-cn/奇异值分解" target="_blank">Wikipedia</a>上的例子: 矩阵
      \[A = \begin{bmatrix}
      1 & 0 & 0 & 0 & 2 \\
      0 & 0 & 3 & 0 & 0 \\
      0 & 0 & 0 & 0 & 0 \\
      0 & 4 & 0 & 0 & 0
      \end{bmatrix}\]
      可以被分解为 \(A = U \Sigma V^{*}\) , 其中
      \[\begin{aligned}
      U & = \begin{bmatrix}
      0 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 0 & 1 \\
      1 & 0 & 0 & 0
      \end{bmatrix}, \\
      \Sigma & = \begin{bmatrix}
      4 & 0 & 0 & 0 & 0 \\
      0 & 3 & 0 & 0 & 0 \\
      0 & 0 & \sqrt{5} & 0 & 0 \\
      0 & 0 & 0 & 0 & 0
      \end{bmatrix}, \\
      V^{*} & = \begin{bmatrix}
      0 & 1 & 0 & 0 & 0 \\
      0 & 0 & 1 & 0 & 0 \\
      \sqrt{0.2} & 0 & 0 & 0 & \sqrt{0.8} \\
      0 & 0 & 0 & 1 & 0 \\
      \sqrt{0.8} & 0 & 0 & 0 & -\sqrt{0.2}
      \end{bmatrix}. \end{aligned}\]
      然后计算 \(U \Sigma\) 和 \(\Sigma V^{*}\) , 并分别与 \(U\) 和 \(V^{*}\) 比较, 就能看出这点.
    </p>
    <p>
      然后我们先不考虑 \(\Sigma\) , 或者说, 将其非零奇异值先全都换成 \(1\) . 我们假设非零的奇异值有 \(s\) 个, 那么实际上 \(U\) 和 \(V\) 都会退化成只有 \(s\) 列的矩阵, 所以
      \(A' = U'^{m \times s} (V'^{*})^{s \times n}\) (这实际上也能说明SVD中的 \(U\) 和 \(V\) 在 \(\Sigma\) 固定的情况下也很有可能不是唯一的, 因为后面的列只要与前面的列正交就行了).
      我们再将 \(U'\) 和 \(V'^{*}\) 分别按列和行拆开, 拆成 \(U' = \begin{bmatrix} C_1 & C_2 & \cdots & C_s \end{bmatrix}\) 和
      \[V'^{*} = \begin{bmatrix}
      R_1 \\ R_2 \\ \vdots \\ R_s
      \end{bmatrix},\]
      那么如果分块矩阵学得好的话(说实话就是因为我学得不好才一直没发现), 就会发现:
      \[A'^{m \times n} = U' V'^{*} = \sum_{i = 1}^{s} C_i^{m \times 1} R_i^{1 \times n}.\]
      再把 \(\Sigma\) 考虑上, 就会有
      \[A = \sum_{i = 1}^{s} \Sigma_{i, i} C_i R_i.\]
      其中 \(\Sigma_{i, i}\) 是 \(\Sigma\) 中的第 \(i\) 行第 \(i\) 列的元素, 也就是其对角线上的元素. 所以说, SVD相当于是将原来的矩阵拆成了一些矩阵(即 \(C_i R_i\) )的线性组合,
      而 \(\Sigma\) 中的奇异值就是线性组合的系数. 如果有兴趣的话可以再用一次上面的例子进行验证. 
    </p>
    <p>
      又因为 \(U\) 和 \(V\) 的列都是正交的, 所以其分别构成了 \(\mathbb{F}^{m}\) 和 \(\mathbb{F}^{n}\) 上的正交基( \(\mathbb{F}\) 表示 \(\mathbb{R}\) 或者 \(\mathbb{C}\) ),
      这就可以引出另外一种对SVD的解释, 出自Sheldon Axler的<a href="https://linear.axler.net" target="_blank">Linear Algebra Done Right</a>:
      <blockquote>
        <p>7.51 Singular Value Decomposition</p>
        <p>
          Suppose \(T \in \mathscr{L}(V)\) has singular values \(s_1, \cdots, s_n\) .
          Then there exist orthonormal bases \(e_1, \cdots, e_n\) and \(f_1, \cdots, f_n\) of \(V\) such that
          \[T v = s_1 \langle v, e_1 \rangle f_1 + \cdots + s_n \langle v, e_n \rangle f_n\]
          for every \(v \in V\) .
        </p>
      </blockquote>
    </p>
    <h2>SVD是怎么做到压缩图片的?</h2>
    <p>
      简单点说, 把图片的每个通道(比如RGBA)都分别拿出来看, 那么可以将其看作是几个实矩阵, 再做SVD不就行了.
      但是转念一想, 本来只要存一个矩阵, 现在怎么要存三个矩阵了, 数据量不减反增, 这合理吗? 所以关键就在于数据量究竟是多了还是少了, 做道小学数学题就知道了:
      如果只考虑一个通道, 并且所有的数据都以相同的格式存储(也就是说数据的个数决定了数据的大小), 那么将一个 \(m \times n\) 的图片做SVD分解, 需要多少数据? 
    </p>
    <p>
      我们假设有 \(s\) 个奇异值, 那么根据之前的分析, \(U\) 和 \(V\) 都只有前 \(s\) 列是有效的, 所以需要 \(m \times s + s + s \times n = s(m + n + 1)\) 个数据(实际上可以把奇异值提前乘在 \(U\) 或者 \(V\)中, 会少 \(s\) 个数据, 不过这几乎不影响后面的计算).
      那么如果要使得这种操作至少是压缩, 那么就需要 \(s(m + n + 1) \leq m \times n\) , 如果我们再假设 \(m \leq n\) , 那么就有
      \[s \leq \dfrac{m n}{m + n + 1} < \dfrac{m n}{m + m} = \dfrac{n}{2}.\]
      也就是说, 我们大概率需要舍去一些比较小的奇异值, 这也使得基于SVD分解的压缩往往是一种有损压缩.
    </p>
    <p>
      既然是有损压缩的话, 它在每个像素上有多少误差呢? 虽然每个像素的差比较难以给出, 但是, 所有像素的误差的平方和的上界是可以估计的. 根据正交矩阵的性质, \(R_i\) 和 \(C_i\) 都是模为 \(1\) 的向量,
      所以其乘积所得到的矩阵的所有元素的平方和为 \(1\) , 如果考虑上奇异值的放大作用, 平方和就是奇异值的平方.
      从而原像素矩阵和有损压缩后还原的像素矩阵的差矩阵的各元素平方和就会小于等于被舍去的所有奇异值的和的平方(这也可以用上面那个例子做实验).
      说实话, 这个误差看上去蛮大的, 但是我也没有做过实验, 不知道压缩比大概多少. 不过可以肯定的是, 目前比较好的算法可以轻松将PNG压缩成大小30%以内的JPEG, 并且保持其分辨率
      (在<a href="https://squoosh.app/" target="_blank">Squoosh</a>上就可以体验到), 肯定可以吊打朴素的SVD算法.
    </p>
    <h3><del>稍微具体一点的</del>实现思路</h3>
    <p>
      如果完全不在乎运算量的话, SVD怎么算都是可以的, 但是<del>这样的话我就没办法接着水下去了</del>既然能<del>搞</del>算得快点, 为什么不呢?
      首先是计算奇异值, 既然 \(A^{*} A\) 和 \(A A^{*}\) 的特征值是一样的, 那么显然应该挑个阶数小的矩阵计算.
      而特征值怎么计算比较合理呢? 可以考虑 \(Q R\) 迭代, 即先对其做 \(Q R\) 分解, 然后用 \(R Q\) 进行迭代.
      至于这里, 又有一些技巧, 可以先用Householder变换将原矩阵变成三对角矩阵, 然后再用Householder变换做 \(Q R\) 迭代, 可以起到加速的作用.
      求完了特征值, 将其开平方根就会得到奇异值.
      然后, 因为 \(U\) 和 \(V\) 的列向量就是对应的奇异值的平方根的特征向量, 所以利用上面的迭代过程(其本质是对称矩阵的特征分解), 可以直接得到其中一个矩阵.
      最后, 如果已知 \(U\) 的话, 就会有 \(U^{*} A = \Sigma V^{*}\) , 就可以求得 \(V\) , 或者已知 \(V\) 的话, 就有 \(A V = \Sigma U\) , 基本同理.
      最后只要把比较大的奇异值和对应的特征向量存下来就好了.
    </p>
    <p>
      关于 \(Q R\) 分解, Householder变换, \(Q R\) 迭代, 和SVD的详细内容,
      可以见Justin Solomon的<a href="http://people.csail.mit.edu/jsolomon/share/book/numerical_book.pdf" target="_blank">Numerical Algorithms</a>中的5.1 - 5.3, 5.5, 6.4.2, 和7.1.
    </p>
    <p>
      如果我哪天终于开窍了把这东西写出来了(不是指靠调库实现功能的那种), 说不定会再回来更新一下.
    </p>
  </div>
  <div class="mdui-fab-wrapper" id="theme-switch">
    <button class="mdui-fab mdui-ripple mdui-color-theme-accent">
      <i class="mdui-icon material-icons">color_lens</i>
    </button>
  </div>
</body>
</html>